import streamlit as st
import pandas as pd 
import re 
import streamlit.components.v1 as components

pkl_file_path = "Formosan-Mandarin_sent_pairs_139023entries.pkl"

def read_pickle_file(file_path):
  try:
    df = pd.read_pickle(file_path)
    return df 
  except Exception as e:
    st.error(f"Error reading .pkl file: {e}")
    return None

def main():
  st.title("è‡ºç£å—å³¶èª-è¯èªå¥åº«è³‡æ–™é›†")
  st.subheader("Dataset of Formosan-Mandarin sentence pairs")
  st.markdown(
        """
### è³‡æ–™æ¦‚è¦ Data Overview
- ğŸ¢ è³‡æ–™é›†åˆè¨ˆç´„13è¬ç­†å°ç£å—å³¶èª-è¯èªå¥å°
- âš ï¸ æ­¤æŸ¥è©¢ç³»çµ±åƒ…ä¾›æ•™å­¸èˆ‡ç ”ç©¶ä¹‹ç”¨ï¼Œå…§å®¹ç‰ˆæ¬Šæ­¸åŸå§‹è³‡æ–™æä¾›è€…æ‰€æœ‰

### è³‡æ–™ä¾†æº Data Sources
- ä»¥ä¸‹è³‡æ–™ç¶“ç”±ç¶²è·¯çˆ¬èŸ²å–å¾—ã€‚
   + ğŸ¥… ä¹éšæ•™æ: [æ—èªEæ¨‚åœ’](http://web.klokah.tw)
   + ğŸ’¬ ç”Ÿæ´»æœƒè©±: [æ—èªEæ¨‚åœ’](http://web.klokah.tw)
   + ğŸ§— å¥å‹: [æ—èªEæ¨‚åœ’](http://web.klokah.tw)
   + ğŸ”­ æ–‡æ³•: [è‡ºç£å—å³¶èªè¨€å¢æ›¸](https://alilin.apc.gov.tw/tw/)
- è©å…¸è³‡æ–™ä½¿ç”¨`PDFMiner` å°‡2019ç‰ˆçš„PDFæª”è½‰æˆHTMLï¼Œå†ç”¨`BeautifulSoup`æŠ“å–å¥å°ï¼Œå¶çˆ¾æœƒå‡ºç¾æ—èªè·Ÿè¯èªå°ä¸ä¸Šçš„æƒ…å½¢ã€‚è‹¥ç™¼ç¾éŒ¯èª¤ï¼Œè«‹[è¯çµ¡æˆ‘ğŸ“©](https://howard-haowen.rohan.tw/)ã€‚è©å…¸ä¸­é‡è¤‡å‡ºç¾çš„å¥å­å·²å¾è³‡æ–™é›†ä¸­åˆªé™¤ã€‚
   + ğŸ“š [åŸä½æ°‘æ—èªè¨€ç·šä¸Šè©å…¸](https://e-dictionary.apc.gov.tw/Index.htm?fbclid=IwAR18XBJPj2xs7nhpPlIUZ-P3joQRGXx22rbVcUvp14ysQu6SdrWYvo7gWCc)
- å…¶ä»–è¾­å…¸è³‡æ–™ä¾†æº:
   + ğŸ“š [é˜¿ç¾æ—èªè¾­å…¸ Facidol Dict](https://lab.mirusausiliq.com/facidol-dict/)
   + ğŸ“š [é˜¿ç¾æ—èªè¾­å…¸ Facidol Project](https://lab.mirusausiliq.com/facidolproject/)


### æŸ¥è©¢æ–¹æ³• How to Use
- ğŸ”­ éæ¿¾ï¼šä½¿ç”¨å·¦å´æ¬„åŠŸèƒ½é¸å–®å¯éæ¿¾è³‡æ–™ä¾†æº(å¯å¤šé¸)èˆ‡èªè¨€ï¼Œä¹Ÿå¯ä½¿ç”¨è¯èªæˆ–æ—èªé€²è¡Œé—œéµè©æŸ¥è©¢ã€‚
  - ğŸ” é—œéµè©æŸ¥è©¢æ”¯æ´[æ­£å‰‡è¡¨é”å¼](https://zh.wikipedia.org/zh-tw/æ­£åˆ™è¡¨è¾¾å¼)ã€‚
  - ğŸ¥³ æ—èªç¯„ä¾‹: 
    + ä½¿ç”¨`cia *`æŸ¥è©¢å¸ƒè¾²èªï¼Œèƒ½æ‰¾åˆ°åŒ…å«`danumcia`ã€`luduncia`æˆ–`siulcia`ç­‰è©çš„å¥å­ã€‚
    + ä½¿ç”¨`[a-z]{15,}`æŸ¥è©¢ä»»ä½•æ—èªï¼Œèƒ½æ‰¾åˆ°åŒ…å«15å€‹å­—æ¯ä»¥ä¸Šå–®è©çš„å¥å­ï¼Œæ–¹ä¾¿éæ¿¾é•·è©ã€‚
  - ğŸ¤© è¯èªç¯„ä¾‹: 
    + ä½¿ç”¨`^æœ‰ä¸€`æŸ¥è©¢è¯èªï¼Œèƒ½æ‰¾åˆ°ä½¿ç”¨`æœ‰ä¸€å¤©`ã€`æœ‰ä¸€å¡Š`æˆ–`æœ‰ä¸€æ™š`ç­‰è©å‡ºç¾åœ¨å¥é¦–çš„å¥å­ã€‚
    + ä½¿ç”¨`[0-9]{1,}`æŸ¥è©¢è¯èªï¼Œèƒ½æ‰¾åˆ°åŒ…å«ç¾…é¦¬æ•¸å­—çš„å¥å­ï¼Œå¦‚`æˆ‘ä»Šå¹´16æ­²äº†`ã€‚
- ğŸ“š æ’åºï¼šé»é¸æ¨™é¡Œåˆ—ã€‚ä¾‹å¦‚é»é¸`æ—èª`æ¬„ä½æ¨™é¡Œåˆ—å…§çš„ä»»ä½•åœ°æ–¹ï¼Œè³‡æ–™é›†ä¾¿æœƒæ ¹æ“šæ—èªé‡æ–°æ’åºã€‚
- ğŸ’¬ æ›´å¤šï¼šæ–‡å­—é•·åº¦è¶…éæ¬„å¯¬æ™‚ï¼Œå°‡æ»‘é¼ æ»‘åˆ°æ¬„ä½ä¸Šæ–¹å³å¯é¡¯ç¤ºå®Œæ•´æ–‡å­—ã€‚
- ğŸ¥… æ”¾å¤§ï¼šé»é¸è¡¨æ ¼å³ä¸Šè§’â†˜ï¸é€²å…¥å…¨è¢å¹•æ¨¡å¼ï¼Œå†æ¬¡é»é¸â†˜ï¸è¿”å›ä¸»é ã€‚
        """
  )

  df = read_pickle_file(pkl_file_path)

  zh_columns = {
    "Lang_En": "Language",
    "Lang_Ch": "èªè¨€_æ–¹è¨€",
    "Ab": "æ—èª",
    "Ch": "è¯èª",
    "From": "ä¾†æº",
  }
  df.rename(columns=zh_columns, inplace=True)
  source_set = df['ä¾†æº'].unique()
  sources = st.sidebar.multiselect(
    "è«‹é¸æ“‡è³‡æ–™ä¾†æº",
    options=source_set,
    default='è©å…¸',
  )
  langs = st.sidebar.selectbox(
    "è«‹é¸æ“‡èªè¨€",
    options=[
      'é˜¿ç¾', 'æ³°é›…', 'å¸ƒè¾²', 'å¡é‚£å¡é‚£å¯Œ', 'å™¶ç‘ªè˜­',
      'æ’ç£', 'é­¯å‡±', 'æ’’å¥‡èŠé›…', 'è³½å¤', 'è³½å¾·å…‹',
      'é‚µ', 'å‘å—', 'é„’', 'é”æ‚Ÿ', 'å¤ªé­¯é–£',
      'é”æ‚Ÿ'
    ],
  )
  texts = st.sidebar.radio(
    "è«‹é¸æ“‡é—œéµè©æŸ¥è©¢æ–‡å­—é¡åˆ¥",
    options=['è¯èª', 'æ—èª'],
  )

  s_filt = df['ä¾†æº'].isin(sources)

  # select a language 
  if langs == "å™¶ç‘ªè˜­":
    l_filt = df['Language'] == "Kavalan"
  elif langs == "é˜¿ç¾":
    l_filt = df['Language'] == "Amis"
  elif langs == "æ’’å¥‡èŠé›…":
    l_filt = df['Language'] == "Sakizaya"
  elif langs == "é­¯å‡±":
    l_filt = df['Language'] == "Rukai"
  elif langs == "æ’ç£":
    l_filt = df['Language'] == "Paiwan"
  elif langs == "å‘å—":
    l_filt = df['Language'] == "Puyuma"
  elif langs == "è³½å¾·å…‹":
    l_filt = df['Language'] == "Seediq"
  elif langs == "é‚µ":
    l_filt = df['Language'] == "Thao"
  elif langs == "æ‹‰é˜¿é­¯å“‡":
    l_filt = df['Language'] == "Saaroa"
  elif langs == "é”æ‚Ÿ":
    l_filt = df['Language'] == "Yami"
  elif langs == "æ³°é›…":
    l_filt = df['Language'] == "Atayal"
  elif langs == "å¤ªé­¯é–£":
    l_filt = df['Language'] == "Truku"
  elif langs == "é„’":
    l_filt = df['Language'] == "Tsou"
  elif langs == "å¡é‚£å¡é‚£å¯Œ":
    l_filt = df['Language'] == "Kanakanavu"
  elif langs == "è³½å¤":
    l_filt = df['Language'] == "Saisiyat"
  elif langs == "å¸ƒè¾²":
    l_filt = df['Language'] == "Bunun"

  text_box = st.sidebar.text_input('åœ¨ä¸‹æ–¹è¼¸å…¥è¯èªæˆ–æ—èªï¼ŒæŒ‰ä¸‹ENTERå¾Œä¾¿æœƒè‡ªå‹•æ›´æ–°æŸ¥è©¢çµæœ')

  t_filt = df[texts].str.contains(text_box, flags=re.IGNORECASE)

  filt_df = df[(s_filt) & (l_filt) & (t_filt)]

  st.markdown(
    """
### æŸ¥è©¢çµæœ Result
    """
  )

  st.dataframe(filt_df, 800, 400)

  st.markdown(
    """
### ç‰ˆæ¬Šè²æ˜ License
- é€™å€‹ repo ä¸­çš„ç¨‹å¼ç¢¼æ˜¯åŸºæ–¼ä¸¦ä¿®æ”¹è‡ª [howard-haowen/Formosan-languages](https://github.com/howard-haowen/Formosan-languages/) åŸå§‹ç¢¼ï¼Œç”± [howard-haowen](https://github.com/howard-haowen/) æ‰€å‰µå»ºï¼Œå°æ–¼åŸä½œè€…çš„è²¢ç»è¡¨ç¤ºæ„Ÿè¬ã€‚
- å› åŸç¶²ç«™ç¨‹å¼è¨±ä¹…æœªæ›´æ–°ï¼Œå°è‡´éŒ¯èª¤ç™¼ç”Ÿï¼ŒåŸ repo ä¸­çš„ç¨‹å¼ç¢¼å·²ç„¡æ³•æ­£å¸¸é‹ä½œï¼Œå› æ­¤æˆ‘å°‡å…¶ä¿®æ”¹ç‚ºå¯æ­£å¸¸é‹ä½œçš„ç‰ˆæœ¬ã€‚ 
- The code in this repository is based on and modified from the original source available on [howard-haowen/Formosan-languages](https://github.com/howard-haowen/Formosan-languages/). The original code is the work of [howard-haowen](https://github.com/howard-haowen/), and I would like to express my appreciation for their contributions.
- Due to the prolonged lack of updates to the original website's code, errors have occurred, rendering the code in the original repository unable to function properly. Therefore, I have modified it to a version that functions correctly.
- 2023-2024 Create by [Mirusa Usiliq](https://github.com/mirusausiliq/) & [Repo](https://github.com/mirusausiliq/Formosan-languages-new/)
    """
  )


if __name__ == "__main__":
    main()

